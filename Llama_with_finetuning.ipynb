{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10855657,"sourceType":"datasetVersion","datasetId":6742823}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:57:22.831196Z","iopub.execute_input":"2025-02-28T03:57:22.831499Z","iopub.status.idle":"2025-02-28T03:57:23.201810Z","shell.execute_reply.started":"2025-02-28T03:57:22.831473Z","shell.execute_reply":"2025-02-28T03:57:23.200822Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/transcripts/transcription.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers torch accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:57:24.820688Z","iopub.execute_input":"2025-02-28T03:57:24.821202Z","iopub.status.idle":"2025-02-28T03:57:29.814589Z","shell.execute_reply.started":"2025-02-28T03:57:24.821167Z","shell.execute_reply":"2025-02-28T03:57:29.813431Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install torch transformers datasets accelerate peft bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:57:29.815978Z","iopub.execute_input":"2025-02-28T03:57:29.816365Z","iopub.status.idle":"2025-02-28T03:57:36.589846Z","shell.execute_reply.started":"2025-02-28T03:57:29.816329Z","shell.execute_reply":"2025-02-28T03:57:36.588865Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\ndef extract_text_from_url(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, \"html.parser\")\n\n    # Extract text from paragraphs and headings\n    text = \"\\n\".join([p.get_text() for p in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])])\n    return text\n\n# Example usage\nurls = [\n    \"https://www.geeksforgeeks.org/basics-of-computer-programming-for-beginners/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/introduction-to-computer-graphics/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/computer-science-programming-for-kids/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/introduction-to-programming-languages/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/go-programming-language-introduction/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/introduction-to-data-structures/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/introduction-to-algorithms/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/computer-fundamentals-tutorial/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/c-language-introduction/?ref=gcse_outind\",\n    \"https://www.geeksforgeeks.org/basics-of-computer-and-its-operations/?ref=gcse_outind\"\n]\n\ndata = [{\"text\": extract_text_from_url(url)} for url in urls]\n\n# Save to JSONL\nimport json\nwith open(\"scraped_data.jsonl\", \"w\") as f:\n    for entry in data:\n        f.write(json.dumps(entry) + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:57:36.591497Z","iopub.execute_input":"2025-02-28T03:57:36.591787Z","iopub.status.idle":"2025-02-28T03:57:39.209445Z","shell.execute_reply.started":"2025-02-28T03:57:36.591765Z","shell.execute_reply":"2025-02-28T03:57:39.208649Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces/newlines\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)  # Remove non-ASCII characters\n    return text.strip()\n\n# Apply to dataset\ncleaned_data = [{\"text\": clean_text(entry[\"text\"])} for entry in data]\n\n# Save cleaned text\nwith open(\"cleaned_data.jsonl\", \"w\") as f:\n    for entry in cleaned_data:\n        f.write(json.dumps(entry) + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:57:39.210703Z","iopub.execute_input":"2025-02-28T03:57:39.211296Z","iopub.status.idle":"2025-02-28T03:57:39.248718Z","shell.execute_reply.started":"2025-02-28T03:57:39.211258Z","shell.execute_reply":"2025-02-28T03:57:39.247932Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\nfrom peft import get_peft_model, LoraConfig, TaskType\nhf_token = \"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\"\n\n# 🟢 Load Model & Tokenizer with `hf_token`\nmodel_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,  # Enables mixed precision\n    token=hf_token,             # Use Hugging Face authentication token\n    device_map=\"auto\"           # Automatically assigns GPUs\n)\n\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# 🟢 Updated LoRA Configuration\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # GPT-style model\n    r=8,                            # LoRA rank (adjust as needed)\n    lora_alpha=32,                  # Alpha scaling\n    lora_dropout=0.1,                # Dropout to prevent overfitting\n    bias=\"none\",                     # No additional biases\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # Shows how many parameters are trainable\n\n# 🟢 Load Dataset (Make sure `cleaned_data.jsonl` exists)\ndataset = load_dataset(\"json\", data_files=\"cleaned_data.jsonl\")\n\n# Tokenize the Dataset\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=1024)\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# Split into training and evaluation sets\nsplit_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n\n# 🟢 Training Arguments (Optimized for LoRA)\ntraining_args = TrainingArguments(\n    output_dir=\"./llama-lora-finetuned\",\n    run_name=\"llama_lora_experiment\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=2,   # Reduce if OOM (GPU-dependent)\n    per_device_eval_batch_size=2,\n    num_train_epochs=3,              # Can be increased for better performance\n    learning_rate=2e-4,              # LoRA allows for higher LR\n    weight_decay=0.01,\n    logging_steps=500,\n    fp16=True,\n    push_to_hub=False\n)\n\n# 🟢 Trainer with LoRA\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n\n# Start LoRA Fine-Tuning\ntrainer.train()\n\n# Save LoRA Fine-Tuned Model\nmodel.save_pretrained(\"./llama-lora-finetuned\")\ntokenizer.save_pretrained(\"./llama-lora-finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:57:39.249975Z","iopub.execute_input":"2025-02-28T03:57:39.250315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481ed29e828544b59c3f7c9787429579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2094ba17a4b43b693c6d3965d655457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20258974ea02439b9b107ba71ce49040"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/5.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09c6da2a84c441bbb429421f392db77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/89.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4ee0cd57a94b488d8f6e5b29789027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66e928de305418682a59f3b6e55459e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d714ed0b5a4088a096758d98fbb7cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70dee6a5bd224d989fdbee239af2b1cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269535a86bcc44a989dae9a8b35a0cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5950305487a44579eb44c2305393bae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c9dca891924752bbd48efd03eaa871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b629d6fc5bc0439c9bd2e02bc8aab323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"785bccc0a53b4c67b2691f3c9257f038"}},"metadata":{}},{"name":"stdout","text":"trainable params: 26,214,400 || all params: 9,801,406,480 || trainable%: 0.2675\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b4157f3ea14d90b549c3bb014de901"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31679d4dd98f4859b17520ba7b6a9809"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def generate_study_material(subject, topic, purpose, lecture):\n    prompt = f\"\"\"\n    You are an expert educator who simplifies complex topics for students. Your goal is to **generate a detailed, coherent, and structured study material based on the given lecture transcript.**\n\n    **Ensure that your explanation is strictly aligned with the lecture content while also expanding upon key ideas, providing clarity, and using examples to illustrate concepts.**\n\n    ---\n    \n    ### **Subject:** {subject}  \n    ### **Topic:** {topic}  \n    ### **Purpose:** {purpose}  \n\n    ---\n    \n    ### **Guidelines for Output:**\n    - **Faithfully follow the lecture transcript** while ensuring clarity and depth.\n    - **Explain key concepts in an engaging, structured, and beginner-friendly manner.**\n    - **Use real-world examples, analogies, and step-by-step breakdowns** for better understanding.\n    - **If relevant, include small code snippets (Python, R, SQL, etc.), diagrams, or formulas.**\n    - **Summarize key takeaways at the end** for easy revision.\n    \n    ---\n    \n    ### **Lecture Transcript:**  \n    {lecture}\n    \n    ---\n    \n    ### **Expected Output Format:**  \n    1. **Introduction** – Explain the topic in simple terms and its significance.  \n    2. **Concept Breakdown** – Follow the lecture's flow, expanding ideas where needed.  \n    3. **Real-Life Examples & Analogies** – Make abstract ideas more relatable.  \n    4. **Formulas & Problem-Solving Approaches (if applicable)** – Define key rules, methods, or logic.  \n    5. **Code Snippets (if relevant)** – Provide Python, R, or SQL examples where helpful.  \n    6. **Diagrams or ASCII Illustrations (if applicable)** – Use simple visuals to clarify concepts.  \n    7. **Key Takeaways** – A concise summary of the most important points.  \n    8. **Practice Questions or Thought-Provoking Exercises** – Help reinforce learning.  \n    \n    \"\"\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    output = model.generate(\n        **inputs,\n        max_new_tokens=4096,  \n        temperature=0.7,       \n        top_p=0.9,             \n        do_sample=True,        \n        repetition_penalty=1.1, \n        eos_token_id=tokenizer.eos_token_id  \n    )\n\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    # Remove potential echoing of the input prompt\n    cleaned_output = generated_text.replace(prompt.strip(), \"\").strip()\n\n    return cleaned_output\n\n# Example Usage\nsubject = \"Computer Science\"\ntopic = \"Introduction to Computation and Programming\"\npurpose = \"Convert lecture notes into structured, easy-to-understand study material while staying true to what was taught.\"\n\n# Read lecture transcript\nwith open(\"/kaggle/input/transcripts/transcription.txt\", \"r\", encoding=\"utf-8\") as file:\n    lecture = file.read()\n\n# Generate study material\nstudy_material = generate_study_material(subject, topic, purpose, lecture)\n\n# Print the result\nprint(study_material)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}